# Hint-before-Solving Prompting (HSP)
This is the Source Code of Paper: [Hint-before-Solving Prompting: Guiding LLMs to
Effectively Utilize Encoded Knowledge](https://arxiv.org/).


## What is Hint-before-Solving Prompting (HSP)?

1. Introducing hints can assist LLMs in generating accurate and logical reasoning. As illustrated in the figure below, introducing hints enabled Llama2-70B to reason correctly and obtain the correct answer.

<div align=center>
<img src="./img/intro.png" width="400">
</div>

2. However, providing helpful hints for each test sample is impractical. Can Large Language Models (LLMs) generate useful hints by themselves?
To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas)  for solving the problem and then generate solutions containing intermediate reasoning steps. 
Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings, as shown in the following figure.

<div align=center>
<img src="./img/framework.png" width="650" class="center">
</div>

3. How do the hints generated by LLMs impact reasoning?
The case analysis shown in the following figure shows that LLMs can effectively generate knowledge or key ideas to solve problems, achieving accurate and logical reasoning and obtaining the correct answers.

<div align=center>
<img src="./img/cases.png" width="550" class="center">
</div>


## Get started
We suggest using miniconda/conda to set up the environment based on Python 3.10. 
After creating your env, you'll need to install Pytorch, Transformers, and film.
```
conda activate your_env
pip install torch torchvision torchaudio
pip install transformers
pip install vllm

```

 

## Repo Structure
- `data/`: Include six evaluation datasets. 
- `icl_robust/`: Demonstrated examples for robust analysis.
- `prompt/`:  Applying HSP to SD, LtM, PS, and CoT promptings for the LLMs to generate the reasoning chain.
- `sft/`:
  - `deepspeed_config.json`: We use deepspeed to accelerate model training.
  - `sft_data_*.jsonl`: Supervise fine-tuning datasets.
  - `sft_train.py`: The main train class.
  - `sft_train.sh`: Run this file to train model.  
- `evaluate.py`: Use this script to evaluate the model output file to get accuracy.
- `inference.py`: Run the model to make predictions on a dataset.
- `produce_hintV2.py`: The output is divided into two stages: the first generates a hint, and the second generates a solution.
- `produce_hintV2_gpt.py`: Use gpt4's hint to generate solution.
- `math/`: evaluation of Math datasets.
  - `data/`: Math datasets.
  - `prompt/`:  The prompts used in the experiment.
  -  `evaluate.py`: evaluation script.
  -  `produce.py`: inference script.
  -  `utils.py`: were used for handling data.


## Usage

### Make predictions

Run `inference.py`:

Example:
```
python inference.py --model {model_path} --tp_degree 4 --dataset_name GSM8K --output_path gsm8k_output.json --hint
```

### Evaluate the model predictions
Run `evaluate.py`

The accuracy will be printed to standard output.

Example:
```
python evaluate.py --dataset_name GSM8K --file gsm8k_output.json
```

### SFT on HSPMATH dataset
- We constructed a HSPMATH1 dataset with 7.5k pieces of data based on the gsm8k train dataset with the assistance of GPT-4.
- Based on rewriting questions from the MetaMathQA dataset, we expanded our HSPMATH dataset to 75,000 samples (./sft/sft_data_v2. jsonl).
  - Expansion method: Extract data from the metamath dataset with the same question and answer as the HSPMATH1 dataset and apply the hint of HSPMATH1 to the corresponding problem data.
- We conducted SFT training on 8xA100 (80G) GPUs using the Slurm scheduling system. The training script path is: ./sft/sft_train.sh
  - Please modify the saving path of ckpt: epoch_output_dir_llemma_7b


## Citation
If you find this repository useful, please cite our paper:
```
@article{,
  title={Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge},
  author={},
  journal={},
  year={}
}
```